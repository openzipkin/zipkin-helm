---
apiVersion: v1
kind: Pod
metadata:
  name: "{{ include "zipkin.fullname" . }}-test-connection"
  labels: {}  # we don't need any labels in the test pod!
  annotations:
    "helm.sh/hook": test
spec:
  containers:
{{- if .Values.zipkin.discovery.eureka }}
    - name: verify-eureka
      image: "{{ .Values.testImage.registry }}/{{ .Values.testImage.repository }}:{{ .Values.testImage.tag }}"
      command: [ '/bin/sh', '-c' ]
      # Make sure zipkin registered in Eureka at startup
      args: [ 'wget -q --spider http://{{ include "zipkin.fullname" . }}:8761/eureka/v2/apps/ZIPKIN' ]
{{- end }}
{{ if .Values.global.testDependencies }}
    - name: get-frontend
      image: "{{ .Values.testImage.registry }}/{{ .Values.testImage.repository }}:{{ .Values.testImage.tag }}"
      command: [ '/bin/sh', '-c' ]
      args:  # Get the frontend service 3 times, after it is ready
        - >
           while ! wget -q --spider http://{{ include "zipkin.fullname" . }}:8081/health;do sleep 1; done &&
           for i in 1 2 3;do wget -q --spider http://{{ include "zipkin.fullname" . }}:8081; done
    - name: get-dependencies
      image: "{{ .Values.testImage.registry }}/{{ .Values.testImage.repository }}:{{ .Values.testImage.tag }}"
      command: [ '/bin/sh', '-c' ]
      args:  # Sleep for the trace to process, then get dependencies based on it
        - >
          echo '[{"parent":"frontend","child":"backend","callCount":3}]' > want.json &&
          sleep 3 &&
          wget -qO have.json --header "b3: 0" http://{{ include "zipkin.fullname" . }}:{{ .Values.service.port }}/api/v2/dependencies?endTs=$(( $(date +%s) * 1000 )) &&
          diff -b want.json have.json
{{ end }}
    - name: get-api
      image: "{{ .Values.testImage.registry }}/{{ .Values.testImage.repository }}:{{ .Values.testImage.tag }}"
      command: [ '/bin/sh', '-c' ]
      # Get an arbitrary API endpoint using the ClusterIP and service port
      args: [ 'wget -q --spider --header "b3: cafebabecafebabe-cafebabecafebabe-1" http://{{ include "zipkin.fullname" . }}:{{ .Values.service.port }}/api/v2/services' ]
{{- if .Values.zipkin.selfTracing.enabled }}
    - name: get-self-trace
      image: "{{ .Values.testImage.registry }}/{{ .Values.testImage.repository }}:{{ .Values.testImage.tag }}"
      command: [ '/bin/sh', '-c' ]
      # If self-tracing, sleep for the trace to process. Then, get it by the constant ID passed above.
      args: [ 'sleep 3 && wget -q --spider http://{{ include "zipkin.fullname" . }}:{{ .Values.service.port }}/api/v2/trace/cafebabecafebabe' ]
{{- end }}
{{- if .Values.serviceMonitor.enabled }}
    # This verifies prometheus scraped the zipkin service on the correct
    # endpoint, by reading an actual statistic.
    # See https://prometheus.io/docs/prometheus/latest/querying/api/
    - name: get-prometheus-query
      image: "{{ .Values.testImage.registry }}/{{ .Values.testImage.repository }}:{{ .Values.testImage.tag }}"
      command: [ '/bin/sh', '-c' ]
      # Note: The below commands use the Prometheus API, which returns HTTP 200
      # even on empty. Rather than install jq, we use grep to ensure a result.
      #
      # We use a sleep loop despite the scrape delay of only 1s. This is due to
      # an up to one-minute read-back delay between adding the service monitor,
      # and being visibility as a prometheus target in kube-prometheus-stack.
      args: [ 'until (wget -q -O - http://prometheus-operated.{{ .Values.serviceMonitor.namespace }}.svc.cluster.local:9090/api/v1/query?query=http_server_requests_seconds_max|grep zipkin); do sleep 3; done' ]
{{- end }}
  restartPolicy: Never
